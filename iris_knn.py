# -*- coding: utf-8 -*-
"""Iris_kNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZyGSE0GHRwclA3qixGX66hnfRytol-jY

Data Formating
"""

# Commented out IPython magic to ensure Python compatibility.
#importing dependencies
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

# %matplotlib inline

"""Load the data from scikit learn library"""

from sklearn import linear_model
from sklearn.datasets import load_iris

#load the data
iris = load_iris()

#grab the features (X) and the target (y)
x = iris.data
y = iris.target

# show the built-in data description
print(iris.DESCR)

"""Put the data into pandas DataFrame"""

#grab data
iris_data = DataFrame(x, columns=['Sepal Lenght', 'Sepal Width', 'Petal Lenght', 'Petal Width'])

#grab target
iris_target = DataFrame(y, columns=['Species'])

"""Since species are defined as 0,1 and 2 so we need to rename them. For we can use apply(), to spit the column make a scheme function them combine it back together """

def flower(flower):
  if flower == 0:
    return 'Setosa'
  elif flower == 1:
    return 'Veriscolour'
  else:
    return 'Verginica'

#apply
iris_target['Species'] = iris_target['Species'].apply(flower)

#check
iris_target.head()

iris_data.tail()

iris_target.tail()

#Create a combined Iris DataSet
iris = pd.concat([iris_data, iris_target], axis = 1)

#review Data
iris.head()

"""Data Visualisation """

#Pairplot of all the different features
sns.pairplot(iris, hue = 'Species', size = 2)

"""Examine petal length of all the species"""

sns.factorplot('Petal Length', data = iris, hue = 'Species', size = 2)

"""## Multi-Class Classification with Sci Kit Learn

Multi-Class Classification using kNN Technique. We have to split the data into Testing and Training sets. Then pass a test_size argument to have the testing data be 40% of the total data set. Also pass a random seed number.

testing accuracy with the testing set. We'll make a prediction using our model and then check its accuracy
"""

#Import from SciKit Learn
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# Split the data into Trainging and Testing sets
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4,random_state=3)

# checking first for k=6

# Import the kNeighbors Classifiers 
knn = KNeighborsClassifier(n_neighbors = 6)

# Fit the data
knn.fit(X_train,Y_train)

# Run a prediction
Y_pred = knn.predict(X_test)

# Check Accuracy against the Testing Set
print metrics.accuracy_score(Y_test,Y_pred)

"""Let's see what happens if we reduce that value to k=1, that means the closest point in the feature space to our testing data point will be the class the testing point joins."""

# Import the kNeighbors Classifiers 
knn = KNeighborsClassifier(n_neighbors = 1)

# Fit the data
knn.fit(X_train,Y_train)

# Run a prediction
Y_pred = knn.predict(X_test)

# Check Accuracy against the Testing Set
print metrics.accuracy_score(Y_test,Y_pred)

"""Let's do it for cycle through various k values and find the optimal value."""

# Test k values 1 through 20
k_range = range(1, 21)

# Set an empty list
accuracy = []

# Repeat above process for all k values and append the result
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, Y_train)
    Y_pred = knn.predict(X_test)
    accuracy.append(metrics.accuracy_score(Y_test, Y_pred))

"""Now let's plot the results!"""

plt.plot(k_range, accuracy)
plt.xlabel('K value for for kNN')
plt.ylabel('Testing Accuracy')

